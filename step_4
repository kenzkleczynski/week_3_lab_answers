# %% Imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# %% COLLEGE DATA SET ____________________________________________________________________________________________
# Load college data from URL
def load_college_data(url):
    college = pd.read_csv(url)
    return college


# %% Convert columns to right data types
def convert_college_types(df):
    df = df.copy()
    cols = ["level", "control", "hbcu", "flagship"]
    df[cols] = df[cols].astype('category')
    df["vsa_year"] = df["vsa_year"].astype('Int64')
    return df


# %% One-hot encode categorical variables
def encode_college_features(df):
    df = df.copy()
    cols = ["level", "control", "hbcu", "flagship"]
    df_encoded = pd.get_dummies(df, columns=cols)
    return df_encoded


# %% Drop unnecessary/incomplete columns
def drop_college_columns(df):
    df = df.copy()
    vsa_cols = ["vsa_year", "vsa_grad_after4_first", 
                "vsa_grad_elsewhere_after4_first", "vsa_enroll_after4_first",
                "vsa_enroll_elsewhere_after4_first", "vsa_grad_after6_first",
                "vsa_grad_elsewhere_after6_first", "vsa_enroll_after6_first",
                "vsa_enroll_elsewhere_after6_first", "vsa_grad_after4_transfer",
                "vsa_grad_elsewhere_after4_transfer", "vsa_enroll_after4_transfer",
                "vsa_enroll_elsewhere_after4_transfer", "vsa_grad_after6_transfer",
                "vsa_grad_elsewhere_after6_transfer", "vsa_enroll_after6_transfer", 
                "vsa_enroll_elsewhere_after6_transfer"]
    
    identifier_cols = ["index", "unitid", "chronname", "long_x", "lat_y",
                       "site", "similar", "nicknames", "counted_pct"]
    
    df = df.drop(vsa_cols + identifier_cols, axis=1)
    return df


# %% Create binary target variable for financial aid (1 = high aid, 0 = low aid)
def create_college_target(df, threshold=9343):
    df = df.copy()
    df['aid_f'] = pd.cut(df.aid_value, bins=[-1, threshold, 50000], labels=[0, 1])
    df['aid_f'] = df['aid_f'].astype('Int64')
    prevalence = (df.aid_f.value_counts()[1] / len(df.aid_f))
    print(f"Prevalence: {prevalence:.2%}")
    return df


# %% Drop features that cause data leakage or have too much missing data
def clean_college_features(df):
    df = df.copy()
    categorical_drops = ['city', 'state', 'basic']
    percentile_drops = ['awards_per_value', 'awards_per_state_value', 
                        'awards_per_natl_value', 'exp_award_value',
                        'exp_award_state_value', 'exp_award_natl_value',
                        'exp_award_percentile', 'fte_percentile',
                        'med_sat_percentile', 'endow_percentile',
                        'grad_100_percentile', 'grad_150_percentile',
                        'pell_percentile', 'retain_percentile',
                        'ft_fac_percentile']
    leakage_drops = ['aid_value', 'aid_percentile']
    missing_drops = ["med_sat_value", "endow_value"]
    
    all_drops = categorical_drops + percentile_drops + leakage_drops + missing_drops
    df = df.drop(all_drops, axis=1)
    df = df.dropna(subset=["aid_f"])
    return df


# %% Split data into train, tune, and test sets (60/20/20)
def split_college_data(df, train_size=0.6):
    c_train, c_leftover = train_test_split(df, train_size=train_size, 
                                           stratify=df.aid_f)
    c_tune, c_test = train_test_split(c_leftover, train_size=0.5, 
                                      stratify=c_leftover.aid_f)
    
    print(f"Train prevalence: {(c_train.aid_f.mean() * 100):.2f}%")
    print(f"Tune prevalence: {(c_tune.aid_f.mean() * 100):.2f}%")
    print(f"Test prevalence: {(c_test.aid_f.mean() * 100):.2f}%")
    
    return c_train, c_tune, c_test


# %% Normalize continuous variables using MinMaxScaler
def normalize_college_data(c_train, c_tune, c_test, target_col='aid_f'):
    c_train = c_train.copy()
    c_tune = c_tune.copy()
    c_test = c_test.copy()
    
    numeric_cols = list(c_train.select_dtypes('number').columns)
    numeric_cols = [col for col in numeric_cols if col != target_col]
    
    scaler = MinMaxScaler()
    scaler.fit(c_train[numeric_cols])
    
    c_train[numeric_cols] = scaler.transform(c_train[numeric_cols])
    c_tune[numeric_cols] = scaler.transform(c_tune[numeric_cols])
    c_test[numeric_cols] = scaler.transform(c_test[numeric_cols])
    
    return c_train, c_tune, c_test, scaler


# %% Complete college pipeline
def college_pipeline(url, threshold=9343, train_size=0.6):
    df = load_college_data(url)
    df = convert_college_types(df)
    df = encode_college_features(df)
    df = drop_college_columns(df)
    df = create_college_target(df, threshold)
    df = clean_college_features(df)
    c_train, c_tune, c_test = split_college_data(df, train_size)
    c_train, c_tune, c_test, scaler = normalize_college_data(c_train, c_tune, c_test)

    
    return c_train, c_tune, c_test



# %% JOB DATA SET ____________________________________________________________________________________________
# Load job data from URL
def load_job_data(url):
    job = pd.read_csv(url)
    return job


# %% Convert columns to appropriate data types
def convert_job_types(df):
    df = df.copy()
    cols = ["gender", "ssc_b", "hsc_b", "hsc_s", 
            "degree_t", "workex", "specialisation", "status"]
    df[cols] = df[cols].astype('category')
    df['salary'] = df['salary'].astype("Int64")
    return df


# %% One-hot encode categorical variables
def encode_job_features(df):
    df = df.copy()
    cols = ["gender", "ssc_b", "hsc_b", "hsc_s", 
            "degree_t", "workex", "specialisation", "status"]
    df_encoded = pd.get_dummies(df, columns=cols)
    return df_encoded


# %% Drop unnecessary columns
def drop_job_columns(df):
    df = df.copy()
    identifier_cols = ["sl_no", "salary"]
    one_hot_drops = ["ssc_b_Central", "ssc_b_Others",
                     "hsc_b_Central", "hsc_b_Others", 
                     "hsc_s_Arts", "hsc_s_Commerce", "hsc_s_Science",
                     "degree_t_Comm&Mgmt", "degree_t_Others", "degree_t_Sci&Tech",
                     "workex_No", "workex_Yes",
                     "specialisation_Mkt&Fin", "specialisation_Mkt&HR"]
    
    all_drops = identifier_cols + one_hot_drops
    df = df.drop(all_drops, axis=1)
    df.rename(columns={'status_Not Placed': 'status_Not_Placed'}, inplace=True)
    return df


# %% Create binary target variable for job placement (1 = placed, 0 = not placed)
def create_job_target(df):
    df = df.copy()
    df['placed'] = df['status_Placed'].astype('Int64')
    df = df.drop(['status_Not_Placed', 'status_Placed'], axis=1)
    prevalence = (df.placed.value_counts()[1] / len(df.placed))
    return df


# %% Split data into train, tune, and test sets (60/20/20)
def split_job_data(df, train_size=0.6):
    j_train, j_leftover = train_test_split(df, train_size=train_size, 
                                          stratify=df.placed)
    j_tune, j_test = train_test_split(j_leftover, train_size=0.5, 
                                     stratify=j_leftover.placed)
    
    print(f"Train prevalence: {(j_train.placed.mean() * 100):.2f}%")
    print(f"Tune prevalence: {(j_tune.placed.mean() * 100):.2f}%")
    print(f"Test prevalence: {(j_test.placed.mean() * 100):.2f}%")
    
    return j_train, j_tune, j_test


# %% Normalize continuous variables using MinMaxScaler
def normalize_job_data(j_train, j_tune, j_test, target_col='placed'):
    j_train = j_train.copy()
    j_tune = j_tune.copy()
    j_test = j_test.copy()
    
    numeric_cols = list(j_train.select_dtypes('number').columns)
    numeric_cols = [col for col in numeric_cols if col != target_col]
    
    scaler = MinMaxScaler()
    scaler.fit(j_train[numeric_cols])
    
    j_train[numeric_cols] = scaler.transform(j_train[numeric_cols])
    j_tune[numeric_cols] = scaler.transform(j_tune[numeric_cols])
    j_test[numeric_cols] = scaler.transform(j_test[numeric_cols])
    
    return j_train, j_tune, j_test, scaler


# %% Complete job pipeline
def job_pipeline(url, train_size=0.6):    
    df = load_job_data(url)
    df = convert_job_types(df)
    df = encode_job_features(df)
    df = drop_job_columns(df)
    df = create_job_target(df)
    j_train, j_tune, j_test = split_job_data(df, train_size)
    j_train, j_tune, j_test, scaler = normalize_job_data(j_train, j_tune, j_test)
    
    return j_train, j_tune, j_test




# %% RUN PIPELINES ____________________________________________________________________________________________
# Run College Pipeline
college_url = "https://raw.githubusercontent.com/UVADS/DS-3021/main/data/cc_institution_details.csv"
c_train, c_tune, c_test = college_pipeline(college_url, threshold=9343)
print("college:", c_train.shape, c_tune.shape, c_test.shape)

# Run Job Pipeline
job_url = "https://raw.githubusercontent.com/DG1606/CMS-R-2020/master/Placement_Data_Full_Class.csv"
j_train, j_tune, j_test = job_pipeline(job_url)
print("job:", j_train.shape, j_tune.shape, j_test.shape)


# %%
